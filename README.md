# üß† Fundamentos de Machine Learning y Redes Neuronales

Este repositorio contiene una explicaci√≥n detallada y los conceptos fundamentales detr√°s del proceso de desarrollo y entrenamiento de un modelo de **Inteligencia Artificial** o una **Red Neuronal** desde sus bases.

## üöÄ Proceso de Aprendizaje de una IA (El Ciclo Completo)

El entrenamiento de una IA se basa en un ciclo iterativo y sistem√°tico cuyo objetivo es ajustar los par√°metros internos del modelo para minimizar el error de predicci√≥n.

---

### 1. üìÇ Preparaci√≥n y Divisi√≥n de Datos

El primer paso crucial es la gesti√≥n de los datos de entrada.

* **Dataset (Conjunto de Datos):** La totalidad de los datos disponibles se divide estrat√©gicamente:
    * **Training Set (Conjunto de Entrenamiento):** Se utiliza para que el modelo **aprenda** y ajuste sus pesos (W) y sesgos (b).
    * **Test Set (Conjunto de Prueba):** Se mantiene separado para **evaluar** el rendimiento final del modelo con datos que nunca ha procesado.
    > *Este proceso asegura que el modelo pueda **generalizar** y no solo memorizar.*

---

### 2. üéØ Funci√≥n de P√©rdida (Loss Function) 

[Image of Loss Function Parabola]


La funci√≥n de p√©rdida es la m√©trica matem√°tica que cuantifica el grado de **error** del modelo.

* **Definici√≥n:** Mide la distancia entre el valor predicho ($\hat{y}$) y el valor real ($y$).
* **Objetivo:** El entrenamiento busca encontrar los valores de pesos y sesgos que resulten en el **punto m√≠nimo** de esta funci√≥n. Un valor bajo significa una alta **precisi√≥n** (_accuracy_).

---

### 3. üìâ Optimizaci√≥n: Descenso del Gradiente (Gradient Descent)

El Descenso del Gradiente es el algoritmo usado para navegar la curva de p√©rdida y encontrar ese punto m√≠nimo.

* **Gradiente:** Representa la pendiente de la funci√≥n de p√©rdida. Indica la direcci√≥n y la intensidad del cambio que se necesita.
* **Ajuste:** La IA utiliza el gradiente para determinar c√≥mo y cu√°nto debe **ajustar los pesos y sesgos** en cada iteraci√≥n, movi√©ndose progresivamente hacia el punto de menor error.

---

### 4. ‚öôÔ∏è Entrenamiento en la Red Neuronal

El entrenamiento se divide en dos fases que se ejecutan repetidamente por cada *epoch* (ciclo completo sobre el conjunto de entrenamiento):

#### A. Propagaci√≥n Hacia Adelante (*Forward Pass*)
1.  Los datos pasan a trav√©s de las capas de la red.
2.  En cada neurona, la entrada se multiplica por el **Peso (W)** y se le suma el **Sesgo (b)**.
3.  El resultado se procesa mediante una **Funci√≥n de Activaci√≥n** (como la **Sigmoide** que convierte valores en probabilidades entre 0 y 1).
4.  La red genera una **predicci√≥n** ($\hat{y}$).

#### B. Retropropagaci√≥n (*Backpropagation*) 
Esta es la fase de **aprendizaje** donde el error se propaga hacia atr√°s.

1.  Se calcula la **diferencia** entre la predicci√≥n y el valor real (el error).
2.  La retropropagaci√≥n utiliza el **C√°lculo de Gradientes** para determinar exactamente qu√© neurona o conexi√≥n contribuy√≥ m√°s al error.
3.  Esta informaci√≥n es utilizada por el **Descenso del Gradiente** para realizar la **actualizaci√≥n de pesos y sesgos**, cerrando el ciclo de aprendizaje y optimizando el modelo para la siguiente iteraci√≥n.

---

### 5. ‚úÖ Evaluaci√≥n Final

Tras completar el entrenamiento, el modelo se valida con el **Test Set**.

* La **precisi√≥n** (_accuracy_) y otras m√©tricas se calculan para determinar la capacidad real del modelo para hacer predicciones correctas en datos que no formaron parte de su entrenamiento.

## üõ†Ô∏è Conceptos Clave Adicionales

* **LLM (Large Language Model):** El diagrama hace referencia a que la red neuronal puede ser la base de un LLM que realiza predicciones de palabras.
* **GPU (Unidad de Procesamiento Gr√°fico):** Es esencial para la IA moderna, ya que realiza de manera eficiente los **c√°lculos matriciales** masivos requeridos por el entrenamiento en paralelo.
* **Matrices (W, b):** Los **pesos** y **sesgos** de la red se almacenan como matrices. Son los par√°metros **internos** que la IA ajusta para aprender la relaci√≥n entre los datos.
